{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # Import the re module\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, ElementClickInterceptedException,StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# List to store scraped data\n",
    "list_doc = []\n",
    "\n",
    "# Initialize the WebDriver (update to Edge if needed)\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://avito.ma\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Function to scrape information\n",
    "def scrape_information(xpath):\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.XPATH, xpath))\n",
    "        )\n",
    "        return element.text\n",
    "    except (NoSuchElementException, TimeoutException,StaleElementReferenceException):\n",
    "        return \"NON SPÉCIFIÉ\"\n",
    "        \n",
    "def click_on_elements(path):\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 5).until(  # Increased wait time for stability\n",
    "            EC.element_to_be_clickable((By.XPATH, path))\n",
    "        )\n",
    "        element.click()\n",
    "    except (NoSuchElementException, TimeoutException, ElementClickInterceptedException,StaleElementReferenceException):\n",
    "        print(f\"Could not click on element with path: {path}\")\n",
    "\n",
    "\n",
    "# function pour converter prix format\n",
    "def convert_price_to_mad(price_str, exchange_rate=1):\n",
    "    cleaned_price = re.sub(r'[^\\d]', '', price_str)\n",
    "    price_mad = int(cleaned_price)\n",
    "    converted_price = price_mad * exchange_rate\n",
    "    return converted_price\n",
    "# Select category and confirm\n",
    "click_on_elements(\"//*[@id='__next']/div/div[5]/div[1]/div/button[1]\")\n",
    "click_on_elements(\"//*[@id='__next']/div[2]/div[5]/div[3]/div/div[5]/div/div[4]/label\")\n",
    "click_on_elements(\"/html/body/div[1]/div[2]/div[5]/div[3]/div/div[5]/div/div[4]/div/button[1]\")\n",
    "click_on_elements(\"//*[@id='__next']/div[2]/div[5]/div[3]/div/div[5]/div/div[1]/div/button[1]\")\n",
    "click_on_elements(\"/html/body/div[1]/div[2]/div[5]/div[3]/div/div[7]/button\")\n",
    "\n",
    "\n",
    "# Pagination loop\n",
    "page = 1\n",
    "while True:\n",
    "    print(f\"Scraping page {page}\")\n",
    "\n",
    "    # Loop to scrape multiple listings on each page\n",
    "    for i in range(1, 44):\n",
    "        try:\n",
    "            click_on_elements(f\"//*[@id='__next']/div/main/div/div[5]/div[1]/div/div[1]/a[{i}]\")\n",
    "            time.sleep(3)\n",
    "            current_url = driver.current_url\n",
    "            # Scrape ad details\n",
    "            list_doc.append({\n",
    "                'titre': scrape_information(\"//*[@id='__next']/div/main/div/div[3]/div[1]/div[2]/div[1]/div[1]/div[2]/div[1]/div[1]/div[1]/h1\"),\n",
    "                'prix': convert_price_to_mad(scrape_information(\"//*[@id='__next']/div/main/div/div[3]/div[1]/div[2]/div[1]/div[1]/div[2]/div[1]/div[1]/div[2]/p\")),\n",
    "                'location': scrape_information(\"//*[@id='__next']/div/main/div/div[3]/div[1]/div[2]/div[1]/div[1]/div[2]/div[1]/div[2]/span[1]\"),\n",
    "                'surface': scrape_information(\"/html/body/div[1]/div/main/div/div[3]/div[1]/div[2]/div[1]/div[1]/div[2]/div[4]/div[1]/div[3]/div/span\"),\n",
    "                'nombre de chambres': scrape_information(\"//*[@id='__next']/div/main/div/div[3]/div[1]/div[2]/div[1]/div[1]/div[2]/div[3]/div[1]/div[1]/div/span\"),\n",
    "                'étage': scrape_information(\"/html/body/div[1]/div/main/div/div[3]/div[1]/div[2]/div[1]/div[1]/div[2]/div[4]/div[2]/ol/li[6]/span[2]\"),\n",
    "                'zonage': scrape_information(\"//*[@id='__next']/div/main/div/div[3]/div[1]/div[2]/div[1]/div[1]/div[2]/div[4]/div/ol/li[4]/span[2]\"),\n",
    "                'Secteur':scrape_information(\"/html/body/div[1]/div/main/div/div[3]/div[1]/div[2]/div[1]/div[1]/div[2]/div[4]/div[2]/ol/li[2]/span[2]\"),\n",
    "                'url': current_url\n",
    "            })\n",
    "            print(list_doc[-1])\n",
    "            # # Go back to the previous page\n",
    "            # btn_back_1(\"/html/body/div[1]/div/main/div/div[3]/div[1]/div[2]/div[1]/div[1]/div[1]/div[2]/div[1]/button\",\n",
    "            #            \"//*[@id='__next']/main/div[3]/div/div[1]/a/button\")\n",
    "            driver.back()\n",
    "        except Exception as e:\n",
    "            print(f\"Error with listing {i} on page {page}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Move to the next page\n",
    "    try:\n",
    "        next_button = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//*[@id='__next']/div/main/div/div[5]/div[1]/div/div[2]/div/a[9]\"))\n",
    "        )\n",
    "        next_button.click()\n",
    "        page += 1\n",
    "        time.sleep(3)  # Wait for the new page to load\n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        print(\"No more pages to scrape.\")\n",
    "        break\n",
    " # Sauvegarder les données dans un fichier CSV\n",
    "with open('data_scraped.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = list_doc[0].keys()  # Récupérer les clés du premier élément pour les en-têtes\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(list_doc)\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
